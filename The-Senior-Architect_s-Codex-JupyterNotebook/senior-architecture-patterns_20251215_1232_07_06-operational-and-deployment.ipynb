{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [\ud83c\udfe0 **Home**](NoteBookIndex.ipynb) &nbsp; | &nbsp; [\u23ea **Prev** (04-scalability-and-performance)](senior-architecture-patterns_20251215_1232_06_04-scalability-and-performance.ipynb) &nbsp; | &nbsp; [**Next** (05-messaging-and-communication) \u23e9](senior-architecture-patterns_20251215_1232_08_05-messaging-and-communication.ipynb)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FOLDER: 06-operational-and-deployment\n",
        "**Generated:** 2025-12-15 12:32\n",
        "\n",
        "**Contains:** 4 files | **Total Size:** 0.02 MB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcc2 `06-operational-and-deployment/`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### \ud83d\udcc4 `06-operational-and-deployment/23-blue-green-deployment.md`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# 23\\. Blue-Green Deployment\n",
        "\n",
        "## 1\\. The Concept\n",
        "\n",
        "Blue-Green Deployment is a release strategy that reduces downtime and risk by running two identical production environments, called \"Blue\" and \"Green.\"\n",
        "\n",
        "  * **Blue:** The currently live version (v1) handling 100% of user traffic.\n",
        "  * **Green:** The new version (v2), currently idle or accessible only to internal testers.\n",
        "\n",
        "To release, you deploy v2 to Green, test it thoroughly, and then switch the Load Balancer to route all traffic from Blue to Green. If anything goes wrong, you switch back instantly.\n",
        "\n",
        "## 2\\. The Problem\n",
        "\n",
        "  * **Scenario:** You are deploying a critical update to a banking app.\n",
        "  * **The \"In-Place\" Risk:** You stop the server, unzip the new jar file, and restart the server.\n",
        "      * **Downtime:** The user sees a \"502 Bad Gateway\" for 2 minutes.\n",
        "      * **The Panic:** The new version crashes on startup. You now have to scramble to find the old jar file and redeploy it. The system is down for 15 minutes.\n",
        "      * **The Consequence:** Deployment becomes a scary event that teams avoid doing. \"Don't deploy on Fridays\\!\"\n",
        "\n",
        "## 3\\. The Solution\n",
        "\n",
        "Decouple the \"Deployment\" (installing bits) from the \"Release\" (serving traffic).\n",
        "\n",
        "1.  **Deployment:** You spin up the Green environment. The public cannot see it yet. You run smoke tests against it.\n",
        "2.  **Cutover:** You change the Router/Load Balancer configuration. Traffic flows to Green. Blue is now idle.\n",
        "3.  **Rollback:** If Green throws errors, you just flip the switch back to Blue. It is instantaneous because Blue is still running.\n",
        "\n",
        "### Junior vs. Senior View\n",
        "\n",
        "| Perspective | Approach | Outcome |\n",
        "| :--- | :--- | :--- |\n",
        "| **Junior** | \"I'll use `rsync` to overwrite the files on the live server. It's fast and easy.\" | **Maintenance Windows.** \"The site will be down from 2 AM to 4 AM.\" If the deploy fails, you are stuck debugging live in production. |\n",
        "| **Senior** | \"Infrastructure is disposable. Spin up a completely new stack (Green). Verify it. Switch the pointer. Kill the old stack (Blue) only when we are 100% sure.\" | **Zero Downtime.** Deployments are boring and safe. Rollback is a single button press. We can deploy at 2 PM on a Friday. |\n",
        "\n",
        "## 4\\. Visual Diagram\n",
        "\n",
        "## 5\\. The Hard Part: The Database\n",
        "\n",
        "The infrastructure part is easy (especially with Kubernetes). **The Database is the bottleneck.**\n",
        "\n",
        "  * You usually have **one** shared database for both Blue and Green (syncing two databases in real-time is too complex).\n",
        "  * **The Constraint:** The database schema must be compatible with *both* v1 (Blue) and v2 (Green) at the same time.\n",
        "\n",
        "### The \"Expand-Contract\" Pattern\n",
        "\n",
        "If you need to rename a column from `address` to `full_address`:\n",
        "\n",
        "1.  **Migration 1 (Expand):** Add `full_address` column. Copy data from `address`. Keep `address`.\n",
        "      * *Result:* DB has both. Blue uses `address`. Green uses `full_address`.\n",
        "2.  **Deploy:** Blue-Green Switch.\n",
        "3.  **Migration 2 (Contract):** Once Green is stable, delete the `address` column.\n",
        "\n",
        "## 6\\. When to Use It (and When NOT to)\n",
        "\n",
        "  * \u2705 **Use when:**\n",
        "      * **Critical Uptime:** You cannot afford 5 minutes of downtime.\n",
        "      * **Instant Rollback:** You need a safety net.\n",
        "      * **Monoliths:** It is often easier to Blue/Green a monolith than to do rolling updates.\n",
        "  * \u274c **Avoid when:**\n",
        "      * **Stateful Apps:** If users have active WebSocket connections or in-memory sessions on Blue, switching them to Green cuts them off. (Requires sticky sessions or external session stores like Redis).\n",
        "      * **Destructive DB Changes:** If the new version drops a table, you cannot roll back to Blue (Blue will crash querying the missing table).\n",
        "\n",
        "## 7\\. Implementation Example (Kubernetes)\n",
        "\n",
        "In Kubernetes, this is often done using `Service` selectors.\n",
        "\n",
        "### Step 1: The Current State (Blue)\n",
        "\n",
        "```yaml\n",
        "apiVersion: v1\n",
        "kind: Service\n",
        "metadata:\n",
        "  name: my-app-service\n",
        "spec:\n",
        "  selector:\n",
        "    version: v1  # POINTS TO BLUE\n",
        "  ports:\n",
        "    - port: 80\n",
        "```\n",
        "\n",
        "### Step 2: Deploy Green (v2)\n",
        "\n",
        "We deploy a new Deployment named `app-v2`. It starts up, but receives NO traffic because the Service is still looking for `version: v1`.\n",
        "\n",
        "  * We can port-forward to `app-v2` to test it manually.\n",
        "\n",
        "### Step 3: The Switch\n",
        "\n",
        "We patch the Service to look for `v2`.\n",
        "\n",
        "```bash\n",
        "kubectl patch service my-app-service -p '{\"spec\":{\"selector\":{\"version\":\"v2\"}}}'\n",
        "```\n",
        "\n",
        "  * **Result:** The Service instantly routes new packets to the v2 pods. The v1 pods stop receiving traffic.\n",
        "  * **Cleanup:** After 1 hour, delete the `app-v1` deployment.\n",
        "\n",
        "## 8\\. Blue-Green vs. Canary\n",
        "\n",
        "  * **Blue-Green:** Instant switch. 100% of traffic moves at once. Great for simple applications.\n",
        "  * **Canary:** Gradual shift. 1% -\\> 10% -\\> 50% -\\> 100%. Better for high-scale systems where a bug affecting 100% of users instantly would be catastrophic.\n",
        "\n",
        "## 9\\. Strategic Note on Cost\n",
        "\n",
        "Blue-Green implies running **double the infrastructure** during the deployment window.\n",
        "\n",
        "  * If your production cluster costs $10k/month, you need capacity to spike to $20k/month temporarily.\n",
        "  * **Senior Tip:** In the Cloud, this is cheap (you only pay for the extra hour). On-premise, this is hard (you need double the physical servers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### \ud83d\udcc4 `06-operational-and-deployment/24-canary-release.md`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# 24\\. Canary Release\n",
        "\n",
        "## 1\\. The Concept\n",
        "\n",
        "A Canary Release is a technique to reduce the risk of introducing a new software version in production by slowly rolling out the change to a small subset of users before making it available to everyone. It is named after the \"canary in a coal mine\"\u2014if the canary (the small subset of users) stops singing (encounters errors), you evacuate the mine (rollback) before the miners (the rest of your user base) get hurt.\n",
        "\n",
        "## 2\\. The Problem\n",
        "\n",
        "  * **Scenario:** You have 1 million active users. You deploy version 2.0 using a standard \"Rolling Update\" or \"Blue-Green\" switch.\n",
        "  * **The Bug:** Version 2.0 has a subtle memory leak that only appears under high load, or a UI bug that breaks the \"Checkout\" button for users on iPads.\n",
        "  * **The Impact:** Because you switched 100% of traffic to the new version, **all 1 million users** are affected instantly. Support lines are flooded, revenue drops to zero, and your reputation takes a hit.\n",
        "\n",
        "## 3\\. The Solution\n",
        "\n",
        "Instead of switching 0% to 100%, you switch gradually: 0% -\\> 1% -\\> 10% -\\> 50% -\\> 100%.\n",
        "\n",
        "1.  **Phase 1:** Deploy v2 to a small capacity. Route 1% of live traffic to it.\n",
        "2.  **Verification:** Monitor Error Rates, Latency, and Business Metrics (e.g., \"Orders per minute\").\n",
        "3.  **Expansion:** If metrics are healthy, increase traffic to 10%.\n",
        "4.  **Completion:** Continue until 100% of traffic is on v2. Then decommission v1.\n",
        "\n",
        "### Junior vs. Senior View\n",
        "\n",
        "| Perspective | Approach | Outcome |\n",
        "| :--- | :--- | :--- |\n",
        "| **Junior** | \"We tested it in Staging. It works. Just deploy it to all servers.\" | **High Risk.** Staging is never exactly like Production. Real users do weird things that QA didn't predict. |\n",
        "| **Senior** | \"Staging is a rehearsal. Production is the show. Let 500 random users try the new code first. If they don't complain, let 5,000 try it.\" | **Blast Radius Containment.** If v2 is broken, only 1% of users had a bad day. The other 99% never noticed. We roll back the 1% instantly. |\n",
        "\n",
        "## 4\\. Visual Diagram\n",
        "\n",
        "## 5\\. When to Use It (and When NOT to)\n",
        "\n",
        "  * \u2705 **Use when:**\n",
        "      * **High Scale:** You have enough traffic that \"1%\" is statistically significant.\n",
        "      * **Critical Business Flows:** Changing the Payment Gateway or Login logic.\n",
        "      * **Cloud Native:** You are using Kubernetes, Istio, or AWS ALB, which make weighted routing easy.\n",
        "  * \u274c **Avoid when:**\n",
        "      * **Low Traffic:** If you get 1 request per minute, \"1% traffic\" means waiting 100 minutes for a data point. Just do Blue-Green.\n",
        "      * **Client-Side Apps:** It is harder (though not impossible) to do Canary releases for Mobile Apps (App Store delays) or Desktop software.\n",
        "      * **Database Schema Changes:** Like Blue-Green, Canary requires the database to support *both* versions simultaneously.\n",
        "\n",
        "## 6\\. Implementation Example (Kubernetes/Istio)\n",
        "\n",
        "In a standard Kubernetes setup, you can do a rough Canary by scaling replicas (1 pod v2, 9 pods v1 = 10% traffic).\n",
        "For precise control, you use a Service Mesh like **Istio** or an Ingress Controller like **Nginx**.\n",
        "\n",
        "### Istio `VirtualService` Configuration\n",
        "\n",
        "```yaml\n",
        "apiVersion: networking.istio.io/v1alpha3\n",
        "kind: VirtualService\n",
        "metadata:\n",
        "  name: payment-service\n",
        "spec:\n",
        "  hosts:\n",
        "  - payment-service\n",
        "  http:\n",
        "  - route:\n",
        "    - destination:\n",
        "        host: payment-service\n",
        "        subset: v1  # The Stable Version\n",
        "      weight: 90\n",
        "    - destination:\n",
        "        host: payment-service\n",
        "        subset: v2  # The Canary Version\n",
        "      weight: 10\n",
        "```\n",
        "\n",
        "### The Rollout Strategy (Automated)\n",
        "\n",
        "Manual Canary updates are tedious. Tools like **Flagger** or **Argo Rollouts** automate this:\n",
        "\n",
        "1.  **09:00 AM:** Deploy v2. Flagger sets traffic to 5%.\n",
        "2.  **09:05 AM:** Flagger checks Prometheus: \"Is HTTP 500 rate \\< 1%?\".\n",
        "3.  **09:06 AM:** Success. Flagger increases traffic to 20%.\n",
        "4.  **09:10 AM:** Failure detected (Latency spiked \\> 500ms). Flagger automatically reverts traffic to 0% and sends a Slack alert.\n",
        "\n",
        "## 7\\. What to Monitor (The Canary Analysis)\n",
        "\n",
        "It is not enough to just check \"Is the server up?\" You must compare the **Baseline (v1)** vs. the **Canary (v2)**.\n",
        "\n",
        "1.  **Technical Metrics:**\n",
        "      * HTTP Error Rate (5xx).\n",
        "      * Latency (p99).\n",
        "      * CPU/Memory Saturation.\n",
        "2.  **Business Metrics (The Senior level):**\n",
        "      * \"Add to Cart\" conversion rate.\n",
        "      * \"Ad Impressions\" count.\n",
        "      * *Why?* v2 might be technically \"stable\" (no crashes), but if a CSS bug hides the \"Buy\" button, revenue drops. Only business metrics catch this.\n",
        "\n",
        "## 8\\. Sticky Sessions\n",
        "\n",
        "A common challenge: A user hits the site and gets the Canary (v2). They refresh the page and get the Stable (v1). This is jarring.\n",
        "**Solution:** Enable **Session Affinity** (Sticky Sessions) based on a Cookie or User ID. Once a user is assigned to the Canary group, they should stay there until the deployment finishes.\n",
        "\n",
        "## 9\\. Canary vs. Blue-Green vs. Rolling\n",
        "\n",
        "  * **Rolling Update:** Update server 1, then server 2, etc. (Easiest, but hard to rollback).\n",
        "  * **Blue-Green:** Switch 100% traffic at once. (Safest for rollback, but risky impact).\n",
        "  * **Canary:** Switch traffic gradually. (Safest for impact, but most complex setup)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### \ud83d\udcc4 `06-operational-and-deployment/25-immutable-infrastructure.md`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "# 25\\. Immutable Infrastructure\n",
        "\n",
        "## 1\\. The Concept\n",
        "\n",
        "Immutable Infrastructure is an approach where servers are never modified after they are deployed. If you need to update an application, fix a bug, or apply a security patch, you do not SSH into the server to run `apt-get update`. Instead, you build a completely new machine image (or container), deploy the new instance, and destroy the old one.\n",
        "\n",
        "## 2\\. The Problem\n",
        "\n",
        "  * **Scenario:** You have 20 servers running your application. They were all set up 2 years ago.\n",
        "  * **The Configuration Drift:** Over time, sysadmins have logged in to tweak settings:\n",
        "      * Server 1 has `Java 8u101` and a hotfix for Log4j.\n",
        "      * Server 2 has `Java 8u102` but is missing the hotfix.\n",
        "      * Server 3 has a random cron job installed by an employee who quit last year.\n",
        "  * **The \"Snowflake\" Server:** Each server is unique (a snowflake). If Server 5 crashes, nobody knows exactly how to recreate it because the manual changes weren't documented.\n",
        "  * **The Fear:** \"Don't touch Server 1\\! If you reboot it, it might not come back up.\"\n",
        "\n",
        "## 3\\. The Solution\n",
        "\n",
        "Treat servers like cattle, not pets.\n",
        "\n",
        "1.  **Bake:** Define your server configuration in code (Dockerfile, Packer). Build an image (AMI / Docker Image). This image is now \"frozen\" and immutable.\n",
        "2.  **Deploy:** Launch 20 instances of this exact image.\n",
        "3.  **Update:** To change a configuration, update the code, bake a *new* image (v2), and replace the old instances.\n",
        "4.  **Prohibit SSH:** In extreme implementations, SSH access is disabled. No human *can* change the live server.\n",
        "\n",
        "### Junior vs. Senior View\n",
        "\n",
        "| Perspective | Approach | Outcome |\n",
        "| :--- | :--- | :--- |\n",
        "| **Junior** | \"I'll use Ansible to loop through all 100 servers and update the config file in place.\" | **Drift & Decay.** If the script fails on server \\#42, that server is now inconsistent. The state of the fleet is unknown. |\n",
        "| **Senior** | \"I'll build a new Docker image with the new config. Kubernetes will roll out the new pods and terminate the old ones.\" | **Consistency.** We know exactly what is running in production because it is binary-identical to what we tested in staging. |\n",
        "\n",
        "## 4\\. Visual Diagram\n",
        "\n",
        "## 5\\. When to Use It (and When NOT to)\n",
        "\n",
        "  * \u2705 **Use when:**\n",
        "      * **Cloud / Virtualization:** It requires the ability to provision and destroy VMs/Containers instantly (AWS, Azure, Kubernetes).\n",
        "      * **Scaling:** Auto-scaling groups need a \"Golden Image\" to launch new instances from automatically.\n",
        "      * **Compliance:** You can prove to auditors exactly what software version was running at any point in time by showing the image hash.\n",
        "  * \u274c **Avoid when:**\n",
        "      * **Physical Hardware:** You cannot throw away a physical Dell server every time you update Nginx. (Though you can re-image it via PXE boot, it's slow).\n",
        "      * **Stateful Databases:** You generally *do* patch database servers in place (or rely on managed services like RDS) because moving terabytes of data to a new instance takes too long.\n",
        "\n",
        "## 6\\. Implementation Example (Packer & Terraform)\n",
        "\n",
        "### Step 1: Define the Image (Packer)\n",
        "\n",
        "Create a definition that builds the OS + App dependencies.\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"builders\": [{\n",
        "    \"type\": \"amazon-ebs\",\n",
        "    \"ami_name\": \"my-app-v1.0-{{timestamp}}\",\n",
        "    \"instance_type\": \"t2.micro\",\n",
        "    \"source_ami\": \"ami-12345678\"\n",
        "  }],\n",
        "  \"provisioners\": [{\n",
        "    \"type\": \"shell\",\n",
        "    \"inline\": [\n",
        "      \"sudo apt-get update\",\n",
        "      \"sudo apt-get install -y nginx\",\n",
        "      \"sudo cp /tmp/my-app.conf /etc/nginx/nginx.conf\"\n",
        "    ]\n",
        "  }]\n",
        "}\n",
        "```\n",
        "\n",
        "*Run `packer build` -\\> Output: `ami-0abc123`*\n",
        "\n",
        "### Step 2: Deploy the Image (Terraform)\n",
        "\n",
        "Update your infrastructure code to use the new AMI ID.\n",
        "\n",
        "```hcl\n",
        "resource \"aws_launch_configuration\" \"app_conf\" {\n",
        "  image_id      = \"ami-0abc123\" # The new immutable image\n",
        "  instance_type = \"t2.micro\"\n",
        "}\n",
        "\n",
        "resource \"aws_autoscaling_group\" \"app_asg\" {\n",
        "  launch_configuration = aws_launch_configuration.app_conf.name\n",
        "  min_size = 3\n",
        "  max_size = 10\n",
        "  \n",
        "  # Terraform will gradually replace old instances with new ones\n",
        "}\n",
        "```\n",
        "\n",
        "## 7\\. The Golden Image vs. Base Image\n",
        "\n",
        "  * **Golden Image:** Includes the OS, dependencies, AND the application code.\n",
        "      * *Pros:* Fastest startup (machine is ready to serve traffic immediately).\n",
        "      * *Cons:* Slow build time (every code change requires baking a full VM image).\n",
        "  * **Base Image (Hybrid):** Includes OS + Dependencies (Java/Node). The Application code is downloaded at boot time (User Data).\n",
        "      * *Pros:* Faster CI/CD pipeline.\n",
        "      * *Cons:* Slower startup/scaling time.\n",
        "      * *Senior Choice:* Use **Docker**. The \"Golden Image\" build time for a container is seconds, giving you the best of both worlds.\n",
        "\n",
        "## 8\\. Troubleshooting (The \"Debug Container\" Pattern)\n",
        "\n",
        "If you can't SSH into production, how do you debug a crash?\n",
        "\n",
        "1.  **Centralized Logging:** Logs must be shipped to ELK/Splunk immediately. You debug via logs, not `tail -f`.\n",
        "2.  **Metrics:** Prometheus/Datadog provides the health vitals.\n",
        "3.  **The Sidecar:** In Kubernetes, you can attach a temporary \"Debug Container\" (with curl, netstat, etc.) to the crashing pod to inspect it without modifying the pod itself.\n",
        "\n",
        "## 9\\. Key Benefits Summary\n",
        "\n",
        "1.  **Predictability:** Works in Prod exactly like it worked in Dev.\n",
        "2.  **Security:** If a hacker compromises a server, you don't \"clean\" it. You kill it. The persistence of the malware is limited to the life of that instance.\n",
        "3.  **Rollback:** Switch the Auto Scaling Group back to the previous AMI ID. Done.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### \ud83d\udcc4 `06-operational-and-deployment/README.md`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# \ud83d\udea2 Group 6: Operational & Deployment\n",
        "\n",
        "## Overview\n",
        "\n",
        "**\"It works on my machine\" is not a deployment strategy.**\n",
        "\n",
        "Writing code is the easy part. Getting that code into production reliably, without downtime, and ensuring it runs consistently across 100 servers is the hard part. This module shifts focus from *Code Architecture* to *Infrastructure Architecture*.\n",
        "\n",
        "These patterns move you away from \"Pet\" servers (hand-crafted, fragile) to \"Cattle\" servers (automated, disposable). They introduce safety nets that allow you to deploy at 2 PM on a Friday without fear.\n",
        "\n",
        "## \ud83d\udcdc Pattern Index\n",
        "\n",
        "| Pattern | Goal | Senior \"Soundbite\" |\n",
        "| :--- | :--- | :--- |\n",
        "| **[23. Blue-Green Deployment](https://www.google.com/search?q=./23-blue-green-deployment.md)** | **Zero Downtime** | \"Spin up the new version next to the old one. Switch the traffic instantly. If it breaks, switch back.\" |\n",
        "| **[24. Canary Release](https://www.google.com/search?q=./24-canary-release.md)** | **Risk Reduction** | \"Don't give the new update to everyone. Give it to 1% of users and see if they survive.\" |\n",
        "| **[25. Immutable Infrastructure](https://www.google.com/search?q=./25-immutable-infrastructure.md)** | **Consistency** | \"Never patch a running server. If you need to change a config, build a new image and replace the server.\" |\n",
        "\n",
        "## \ud83e\udde0 The Operational Checklist\n",
        "\n",
        "Before approving a deployment strategy, a Senior Architect asks:\n",
        "\n",
        "1.  **The \"Undo\" Test:** If the deployment fails 30 seconds after go-live, can we revert to the previous version in under 1 minute? (Blue-Green allows this).\n",
        "2.  **The \"Blast Radius\" Test:** If we ship a critical bug, does it take down the entire platform, or just affect a small group? (Canary limits this).\n",
        "3.  **The \"Drift\" Test:** Are the servers running in production exactly the same as the ones we tested in staging? Or has someone manually tweaked the `nginx.conf` on Prod-Server-05? (Immutable Infrastructure prevents this).\n",
        "4.  **The \"Database\" Test:** Does the database schema support *both* the old code and the new code running simultaneously? (Required for all zero-downtime patterns).\n",
        "\n",
        "## \u26a0\ufe0f Common Pitfalls in This Module\n",
        "\n",
        "  * **Infrastructure as ClickOps:** Manually clicking around the AWS Console to create servers. This is unrepeatable and dangerous. Use Terraform/CloudFormation.\n",
        "  * **Ignoring the Database:** Implementing fancy Blue-Green deployments for the code but forgetting that a database migration locks the table for 10 minutes, causing downtime anyway.\n",
        "  * **Lack of Observability:** Doing a Canary release without having the dashboards to actually tell if the Canary is failing.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}