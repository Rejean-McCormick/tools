{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [\ud83c\udfe0 **Home**](NoteBookIndex.ipynb) &nbsp; | &nbsp; [\u23ea **Prev** (07-observability-and-maintenance)](senior-architecture-patterns_20251215_1232_05_07-observability-and-maintenance.ipynb) &nbsp; | &nbsp; [**Next** (06-operational-and-deployment) \u23e9](senior-architecture-patterns_20251215_1232_07_06-operational-and-deployment.ipynb)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FOLDER: 04-scalability-and-performance\n",
        "**Generated:** 2025-12-15 12:32\n",
        "\n",
        "**Contains:** 4 files | **Total Size:** 0.02 MB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcc2 `04-scalability-and-performance/`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### \ud83d\udcc4 `04-scalability-and-performance/17-sharding-partitioning.md`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 17\\. Sharding (Database Partitioning)\n",
        "\n",
        "## 1\\. The Concept\n",
        "\n",
        "Sharding is a method of splitting and storing a single logical dataset (like a \"Users\" table) across multiple databases or machines. By distributing the data, you distribute the load. Instead of one massive server handling 100% of the traffic, you might have 10 servers, each handling 10% of the traffic.\n",
        "\n",
        "## 2\\. The Problem\n",
        "\n",
        "  * **Scenario:** Your application has hit 100 million users.\n",
        "  * **The Vertical Limit:** You have already upgraded your database server to the largest instance available (128 cores, 2TB RAM). It's still hitting 100% CPU during peak hours. You physically cannot buy a bigger computer (Vertical Scaling limit reached).\n",
        "  * **The Bottleneck:** Writes are slow because of lock contention. Indexes are too big to fit in RAM, causing disk thrashing. Backups take 48 hours to run.\n",
        "\n",
        "## 3\\. The Solution\n",
        "\n",
        "Break the database into smaller chunks called **Shards**.\n",
        "Each shard holds a subset of the data. The application uses a **Shard Key** to determine which server to talk to.\n",
        "\n",
        "  * **Shard A:** Users ID 1 - 1,000,000\n",
        "  * **Shard B:** Users ID 1,000,001 - 2,000,000\n",
        "  * ...\n",
        "\n",
        "### Junior vs. Senior View\n",
        "\n",
        "| Perspective | Approach | Outcome |\n",
        "| :--- | :--- | :--- |\n",
        "| **Junior** | \"The database is slow. Let's just add a Read Replica.\" | **Write Bottleneck.** Replicas help with reads, but every write still has to go to the single Master. The Master eventually dies. |\n",
        "| **Senior** | \"We are write-bound. We need to Shard. Let's partition by `RegionID` so users in Europe hit the EU Shard and users in US hit the US Shard.\" | **Linear Scalability.** We can theoretically scale to infinity by just adding more servers. Write throughput multiplies by N. |\n",
        "\n",
        "## 4\\. Visual Diagram\n",
        "\n",
        "## 5\\. Sharding Strategies\n",
        "\n",
        "Choosing the right **Shard Key** is the most critical decision.\n",
        "\n",
        "### A. Range Based (e.g., by User ID)\n",
        "\n",
        "  * *Method:* IDs 1-100 go to DB1, 101-200 go to DB2.\n",
        "  * *Pro:* Easy to implement.\n",
        "  * *Con:* **Hotspots.** If all new users (IDs 900+) are active, and old users (IDs 1-100) are inactive, DB1 is idle while DB9 is melting down.\n",
        "\n",
        "### B. Hash Based (e.g., `hash(UserID) % 4`)\n",
        "\n",
        "  * *Method:* Apply a hash function to the ID to assign it to a server.\n",
        "  * *Pro:* Even distribution of data. No hotspots.\n",
        "  * *Con:* **Resharding is painful.** If you add a 5th server, the formula changes (`% 5`), and you have to move almost ALL data to new locations.\n",
        "\n",
        "### C. Directory Based (Lookup Table)\n",
        "\n",
        "  * *Method:* A separate \"Lookup Service\" tells you where \"User A\" lives.\n",
        "  * *Pro:* Total flexibility. You can move individual users without changing code.\n",
        "  * *Con:* **Single Point of Failure.** If the Lookup Service goes down, nobody can find their data.\n",
        "\n",
        "## 6\\. When to Use It (and When NOT to)\n",
        "\n",
        "  * \u2705 **Use when:**\n",
        "      * **Massive Data:** TBs or PBs of data.\n",
        "      * **Write Heavy:** You have more write traffic than a single node can handle.\n",
        "      * **Geographic Needs:** You want EU user data to physically stay in EU servers (GDPR).\n",
        "  * \u274c **Avoid when:**\n",
        "      * **You haven't optimized queries:** Bad SQL is usually the problem, not the server size. Fix the code first.\n",
        "      * **You need complex Joins:** You cannot easily JOIN tables across two different servers. You have to do it in application code (slow).\n",
        "      * **Small Teams:** The operational complexity of managing 10 databases instead of 1 is huge.\n",
        "\n",
        "## 7\\. Implementation Example (Pseudo-code)\n",
        "\n",
        "**Scenario:** A library wrapper that routes queries to the correct shard based on `user_id`.\n",
        "\n",
        "```python\n",
        "# Configuration: Map shards to connection strings\n",
        "SHARD_MAP = {\n",
        "    0: \"postgres://db-shard-alpha...\",\n",
        "    1: \"postgres://db-shard-beta...\",\n",
        "    2: \"postgres://db-shard-gamma...\"\n",
        "}\n",
        "\n",
        "def get_shard_connection(user_id):\n",
        "    # 1. Determine Shard ID (Hash Strategy)\n",
        "    # Using modulo to distribute users evenly across 3 shards\n",
        "    num_shards = len(SHARD_MAP)\n",
        "    shard_id = hash(user_id) % num_shards\n",
        "    \n",
        "    # 2. Connect to the specific database\n",
        "    connection_string = SHARD_MAP[shard_id]\n",
        "    return connect_to_db(connection_string)\n",
        "\n",
        "def save_user(user):\n",
        "    # The application logic doesn't know about the physical servers.\n",
        "    # It just asks for \"the right connection\".\n",
        "    conn = get_shard_connection(user.id)\n",
        "    \n",
        "    conn.execute(\"INSERT INTO users ...\", user)\n",
        "    conn.close()\n",
        "```\n",
        "\n",
        "## 8\\. The \"Resharding\" Nightmare\n",
        "\n",
        "Eventually, Shard A will get full. You need to split it into Shard A and Shard B.\n",
        "\n",
        "  * **The Senior Reality:** This is terrifying.\n",
        "  * **The Strategy:** Consistent Hashing or Virtual Buckets.\n",
        "      * Instead of mapping `User -> Server`, map `User -> Bucket` (e.g., 1024 buckets).\n",
        "      * Then map `Bucket -> Server`.\n",
        "      * When you add a server, you just move a few buckets over, rather than calculating new hashes for every user.\n",
        "\n",
        "## 9\\. Limitations (The Trade-offs)\n",
        "\n",
        "1.  **No Cross-Shard Transactions:** You cannot start a transaction that updates User A (Shard 1) and User B (Shard 2). You must use **Sagas (Pattern \\#14)**.\n",
        "2.  **No Cross-Shard Joins:** You cannot `SELECT * FROM Orders JOIN Users`. You must fetch User, then fetch Orders, and combine them in Python/Java.\n",
        "3.  **Unique Constraints:** You cannot enforce \"Unique Email\" across the whole system easily, because Shard 1 doesn't know what emails Shard 2 has."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### \ud83d\udcc4 `04-scalability-and-performance/18-cache-aside-lazy-loading.md`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 18\\. Cache-Aside (Lazy Loading)\n",
        "\n",
        "## 1\\. The Concept\n",
        "\n",
        "Cache-Aside (also known as Lazy Loading) is the most common caching strategy. The application logic (\"the Aside\") serves as the coordinator between the data store (Database) and the cache (e.g., Redis/Memcached). The cache does not talk to the database directly. Instead, the application lazily loads data into the cache only when it is actually requested.\n",
        "\n",
        "## 2\\. The Problem\n",
        "\n",
        "  * **Scenario:** You have a high-traffic e-commerce site. The \"Product Details\" page executes complex SQL queries (joins across Pricing, Inventory, and Specs tables).\n",
        "  * **The Reality:** 95% of users are looking at the same 5 popular products (e.g., the latest iPhone).\n",
        "  * **The Performance Hit:** Your database is hammering the disk to calculate the exact same result thousands of times per second. Latency spikes, and the database CPU hits 100%.\n",
        "\n",
        "## 3\\. The Solution\n",
        "\n",
        "Treat the Cache as a temporary key-value storage for the result of those expensive queries.\n",
        "\n",
        "1.  **Read:** When the app needs data, it checks the Cache first.\n",
        "      * **Hit:** Return data immediately (0ms).\n",
        "      * **Miss:** Query the Database, write the result to the Cache, then return data.\n",
        "2.  **Write:** When the app updates data, it updates the Database and **deletes (invalidates)** the Cache entry so the next read forces a fresh fetch.\n",
        "\n",
        "### Junior vs. Senior View\n",
        "\n",
        "| Perspective | Approach | Outcome |\n",
        "| :--- | :--- | :--- |\n",
        "| **Junior** | \"I'll write a script to load *all* our products into Redis when the server starts.\" | **Cold Start & Waste.** Startup takes forever. You fill RAM with data nobody wants (products from 2012). If Redis restarts, the app crashes because the cache is empty. |\n",
        "| **Senior** | \"Load nothing on startup. Let the traffic dictate what gets cached. Set a Time-To-Live (TTL) so unused data naturally drops out of RAM.\" | **Efficiency.** The cache only contains the 'Working Set' (currently popular items). Memory is used efficiently. The system handles empty caches gracefully. |\n",
        "\n",
        "## 4\\. Visual Diagram\n",
        "\n",
        "## 5\\. When to Use It (and When NOT to)\n",
        "\n",
        "  * \u2705 **Use when:**\n",
        "      * **Read-Heavy Workloads:** News sites, blogs, catalogs, social media feeds.\n",
        "      * **General Purpose:** This is the default caching strategy for 80% of web apps.\n",
        "      * **Resilience:** If the Cache goes down, the system still works (just slower) because it falls back to the DB.\n",
        "  * \u274c **Avoid when:**\n",
        "      * **Write-Heavy Workloads:** If data changes every second, you are constantly invalidating the cache. You spend more time writing to Redis than reading from it.\n",
        "      * **Critical Consistency:** If the user *must* see the absolute latest version (e.g., Bank Balance), caching introduces the risk of stale data.\n",
        "\n",
        "## 6\\. Implementation Example (Pseudo-code)\n",
        "\n",
        "**Scenario:** Fetching a User Profile.\n",
        "\n",
        "```python\n",
        "import redis\n",
        "import json\n",
        "\n",
        "# Connection to Cache\n",
        "cache = redis.Redis(host='localhost', port=6379)\n",
        "TTL_SECONDS = 300 # 5 minutes\n",
        "\n",
        "def get_user_profile(user_id):\n",
        "    cache_key = f\"user:{user_id}\"\n",
        "\n",
        "    # 1. Try Cache (The \"Aside\")\n",
        "    cached_data = cache.get(cache_key)\n",
        "    \n",
        "    if cached_data:\n",
        "        print(\"Cache Hit!\")\n",
        "        return json.loads(cached_data)\n",
        "\n",
        "    # 2. Cache Miss - Go to Source of Truth\n",
        "    print(\"Cache Miss - Querying DB...\")\n",
        "    user = db.query(\"SELECT * FROM users WHERE id = ?\", user_id)\n",
        "    \n",
        "    if user:\n",
        "        # 3. Populate Cache (Lazy Load)\n",
        "        # We serialize to JSON because Redis stores strings/bytes\n",
        "        cache.setex(\n",
        "            name=cache_key, \n",
        "            time=TTL_SECONDS, \n",
        "            value=json.dumps(user)\n",
        "        )\n",
        "    \n",
        "    return user\n",
        "\n",
        "def update_user_email(user_id, new_email):\n",
        "    # 1. Update Source of Truth\n",
        "    db.execute(\"UPDATE users SET email = ? ...\", new_email)\n",
        "    \n",
        "    # 2. Invalidate Cache\n",
        "    # Next time someone asks for this user, it will be a \"Miss\"\n",
        "    # and they will fetch the new email from DB.\n",
        "    cache.delete(f\"user:{user_id}\")\n",
        "```\n",
        "\n",
        "## 7\\. The \"Thundering Herd\" Problem (Senior Nuance)\n",
        "\n",
        "There is a specific danger in Cache-Aside.\n",
        "\n",
        "  * **Scenario:** The cache key for \"Homepage\\_News\" expires at 12:00:00.\n",
        "  * **The Spike:** At 12:00:01, you have 5,000 concurrent users hitting the homepage.\n",
        "  * **The Herd:** All 5,000 requests check the cache. All 5,000 get a \"Miss.\" All 5,000 hit the Database simultaneously to generate the same news feed.\n",
        "  * **Result:** The database crashes.\n",
        "\n",
        "**The Senior Fix:** **Locking** or **Probabilistic Early Expiration**.\n",
        "\n",
        "  * *Locking:* Only allow *one* thread to query the DB for \"Homepage\\_News.\" The other 4,999 wait for that thread to finish and populate the cache.\n",
        "  * *Soft TTL:* Tell Redis the TTL is 60s, but tell the App the TTL is 50s. The first user to hit it between 50s and 60s re-generates the cache in the background while everyone else is still served the old (but valid) data.\n",
        "\n",
        "## 8\\. Cache Invalidation Strategies\n",
        "\n",
        "\"There are only two hard things in Computer Science: Cache Invalidation and naming things.\"\n",
        "\n",
        "1.  **TTL (Time To Live):** The safety net. Even if your code fails to delete the key, it will disappear eventually (e.g., 10 minutes). Always set a TTL.\n",
        "2.  **Write-Through (Alternative):** The application writes to the Cache *and* DB simultaneously. Good for read performance, but slower writes.\n",
        "3.  **Delete vs. Update:** In Cache-Aside, prefer **Deleting** the key on update. If you try to **Update** the cache key, you risk race conditions (two threads updating the cache in the wrong order). Deleting is safer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### \ud83d\udcc4 `04-scalability-and-performance/19-static-content-offloading-cdn.md`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# 19\\. Static Content Offloading (CDN)\n",
        "\n",
        "## 1\\. The Concept\n",
        "\n",
        "Static Content Offloading is the practice of moving non-changing files (images, CSS, JavaScript, Videos, Fonts) away from the primary application server and onto a Content Delivery Network (CDN). A CDN is a geographically distributed network of proxy servers. The goal is to serve content to end-users with high availability and high performance by serving it from a location closest to them.\n",
        "\n",
        "## 2\\. The Problem\n",
        "\n",
        "  * **Scenario:** Your application server is hosted in **Virginia, USA (us-east-1)**.\n",
        "  * **The Latency Issue:** A user in **Singapore** visits your site. Every request for `logo.png` or `main.js` has to travel halfway around the world and back. The latency is 250ms+ per file. If your site has 50 files, the page load takes 10+ seconds.\n",
        "  * **The Capacity Issue:** Your expensive App Server (optimized for CPU and Logic) is busy streaming a 50MB video file to a user. During that time, it cannot process login requests or checkout transactions. You are wasting expensive CPU cycles on \"dumb\" file transfer tasks.\n",
        "\n",
        "## 3\\. The Solution\n",
        "\n",
        "Separate the roles:\n",
        "\n",
        "1.  **The App Server:** Handles **Dynamic** content only (JSON, Business Logic, Database interactions).\n",
        "2.  **The CDN:** Handles **Static** content.\n",
        "      * You upload files to \"Object Storage\" (e.g., AWS S3, Google Cloud Storage).\n",
        "      * The CDN (e.g., CloudFront, Cloudflare, Akamai) caches these files at hundreds of \"Edge Locations\" worldwide.\n",
        "      * The user in Singapore downloads the logo from a Singapore Edge Server (10ms latency).\n",
        "\n",
        "### Junior vs. Senior View\n",
        "\n",
        "| Perspective | Approach | Outcome |\n",
        "| :--- | :--- | :--- |\n",
        "| **Junior** | \"I'll put the images in the `/public/images` folder of my Express/Django app and serve them directly.\" | **Server Suffocation.** A viral traffic spike hits. The server runs out of I/O threads serving JPEGs. The API stops responding. The site goes down. |\n",
        "| **Senior** | \"The application server should never serve a file. Push assets to S3 during the build pipeline. Put CloudFront in front. The app server only speaks JSON.\" | **Global Scale.** The static assets load instantly worldwide. The app server is bored and ready to handle business logic. Bandwidth costs drop significantly. |\n",
        "\n",
        "## 4\\. Visual Diagram\n",
        "\n",
        "## 5\\. When to Use It (and When NOT to)\n",
        "\n",
        "  * \u2705 **Use when:**\n",
        "      * **Global Audience:** Users are not physically near your data center.\n",
        "      * **Media Heavy:** The site has large images, videos, or PDFs.\n",
        "      * **High Traffic:** You expect spikes that would crush a single server.\n",
        "      * **Security:** CDNs often provide DDoS protection (WAF) at the edge, shielding your origin server.\n",
        "  * \u274c **Avoid when:**\n",
        "      * **Internal Tools:** An admin panel used by 5 people in the same office as the server.\n",
        "      * **Strictly Dynamic:** An API-only service that serves zero HTML/CSS/Images.\n",
        "\n",
        "## 6\\. Implementation Strategy\n",
        "\n",
        "### Step 1: The Build Pipeline\n",
        "\n",
        "Don't commit binary files to Git if possible. During the deployment process (CI/CD):\n",
        "\n",
        "1.  Build the React/Vue/Angular app.\n",
        "2.  Upload the `./dist` or `./build` folder to an S3 Bucket.\n",
        "3.  Deploy the Backend Code to the App Server.\n",
        "\n",
        "### Step 2: The URL Rewrite\n",
        "\n",
        "In your HTML/Code, you point to the CDN domain, not the relative path.\n",
        "\n",
        "**Before (Junior):**\n",
        "\n",
        "```html\n",
        "<img src=\"/static/logo.png\" />\n",
        "```\n",
        "\n",
        "**After (Senior):**\n",
        "\n",
        "```html\n",
        "<img src=\"https://d12345.cloudfront.net/assets/logo.png\" />\n",
        "```\n",
        "\n",
        "### Step 3: Cache Control (The Critical Header)\n",
        "\n",
        "You must tell the CDN how long to keep the file.\n",
        "\n",
        "  * **Mutable Files (e.g., `index.html`):** Short cache.\n",
        "      * `Cache-Control: public, max-age=60` (1 minute).\n",
        "      * *Reason:* If you deploy a new release, you want users to see it quickly.\n",
        "  * **Immutable Files (e.g., `main.a1b2c3.js`):** Infinite cache.\n",
        "      * `Cache-Control: public, max-age=31536000, immutable` (1 year).\n",
        "      * *Reason:* This file will *never* change. If the code changes, the filename changes (see below).\n",
        "\n",
        "## 7\\. The \"Cache Busting\" Pattern\n",
        "\n",
        "How do we update a file if the CDN has cached it for 1 year?\n",
        "**We don't.** We change the name.\n",
        "\n",
        "  * **Bad:** `style.css`. If you change the CSS and upload it, the CDN might still serve the old one for days.\n",
        "  * **Good (Versioning):** `style.v1.css`, `style.v2.css`.\n",
        "  * **Best (Content Hashing):** `style.8f4a2c.css`.\n",
        "      * Webpack/Vite does this automatically.\n",
        "      * If the file content changes, the hash changes.\n",
        "      * If the hash changes, it's a \"new\" file to the CDN.\n",
        "      * This guarantees that users **never** see a mix of old HTML and new CSS (which breaks layouts).\n",
        "\n",
        "## 8\\. Pseudo-Code Example (S3 Upload Script)\n",
        "\n",
        "```python\n",
        "import boto3\n",
        "import mimetypes\n",
        "import os\n",
        "\n",
        "def deploy_assets_to_cdn(build_folder, bucket_name):\n",
        "    s3 = boto3.client('s3')\n",
        "    \n",
        "    for root, dirs, files in os.walk(build_folder):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            \n",
        "            # Determine Content Type\n",
        "            content_type, _ = mimetypes.guess_type(file_path)\n",
        "            \n",
        "            # Determine Cache Strategy\n",
        "            if file.endswith(\".html\"):\n",
        "                # HTML changes frequently (entry point)\n",
        "                cache_control = \"public, max-age=60\"\n",
        "            else:\n",
        "                # Hash-named assets (JS/CSS/Images) are forever\n",
        "                cache_control = \"public, max-age=31536000, immutable\"\n",
        "\n",
        "            print(f\"Uploading {file} with {cache_control}...\")\n",
        "            \n",
        "            s3.upload_file(\n",
        "                file_path, \n",
        "                bucket_name, \n",
        "                file, \n",
        "                ExtraArgs={\n",
        "                    'ContentType': content_type,\n",
        "                    'CacheControl': cache_control\n",
        "                }\n",
        "            )\n",
        "\n",
        "# Run during CI/CD\n",
        "deploy_assets_to_cdn(\"./build\", \"my-production-assets\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### \ud83d\udcc4 `04-scalability-and-performance/README.md`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# \ud83d\ude80 Group 4: Scalability & Performance\n",
        "\n",
        "## Overview\n",
        "\n",
        "**\"Scalability is the property of a system to handle a growing amount of work by adding resources to the system.\"**\n",
        "\n",
        "In the early days of a startup, you survive on a single server. But as you grow from 1,000 to 1,000,000 users, \"Vertical Scaling\" (buying a bigger CPU) hits a physical wall. You must switch to \"Horizontal Scaling\" (adding more machines).\n",
        "\n",
        "This module covers the strategies Senior Architects use to handle massive traffic and data volume without degrading performance. It focuses on removing bottlenecks at the Database layer, the Application layer, and the Network layer.\n",
        "\n",
        "## \ud83d\udcdc Pattern Index\n",
        "\n",
        "| Pattern | Goal | Senior \"Soundbite\" |\n",
        "| :--- | :--- | :--- |\n",
        "| **[17. Sharding (Partitioning)](https://www.google.com/search?q=./17-sharding-partitioning.md)** | **Horizontal Data Scaling** | \"We can't buy a bigger database server. We must split the users based on Region ID.\" |\n",
        "| **[18. Cache-Aside (Lazy Loading)](https://www.google.com/search?q=./18-cache-aside-lazy-loading.md)** | **Read Optimization** | \"The fastest query is the one you don't make. Check Redis first.\" |\n",
        "| **[19. Static Content Offloading](https://www.google.com/search?q=./19-static-content-offloading-cdn.md)** | **Network Optimization** | \"The application server is for business logic, not for serving 5MB JPEGs. Use a CDN.\" |\n",
        "\n",
        "## \ud83e\udde0 The Scalability Checklist\n",
        "\n",
        "Before launching a marketing campaign or a new feature, a Senior Architect asks:\n",
        "\n",
        "1.  **The \"One Million\" Test:** If we suddenly get 1,000,000 users tomorrow, which component breaks first? (Usually the Database).\n",
        "2.  **The \"Cache Miss\" Test:** If Redis goes down and empties the cache, will the database survive the \"Thundering Herd\" of requests trying to repopulate it?\n",
        "3.  **The \"Physics\" Test:** Are we asking a user in Australia to download a 10MB file from a server in New York? (CDN required).\n",
        "4.  **The \"Hotspot\" Test:** In our sharded database, are 90% of the writes going to Shard A because we chose a bad Shard Key?\n",
        "\n",
        "## \u26a0\ufe0f Common Pitfalls in This Module\n",
        "\n",
        "  * **Caching Everything:** Caching data that changes frequently or is rarely read. You just waste RAM and CPU for serialization.\n",
        "  * **Premature Sharding:** Sharding adds massive operational complexity (backups, resharding, cross-shard joins). Don't do it until you have exhausted Indexing, Read Replicas, and Caching.\n",
        "  * **Ignoring Cache Invalidation:** Showing a user their old bank balance because the cache wasn't cleared after a deposit. This destroys trust.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}